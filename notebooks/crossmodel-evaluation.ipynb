{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961a3731",
   "metadata": {},
   "source": [
    "# Cross-Model Evaluation\n",
    "\n",
    "Comparitive evaluation of the different models trained on different features for Raga Identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f3785",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e94fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c048762",
   "metadata": {},
   "source": [
    "### Load Datasets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adfa3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "Xa = np.load('../dataset/counts.npy')           # Vadi-Samvadi (pitch histogram)\n",
    "Xb = np.load('../dataset/slopes.npy')           # Arohana-Avarohana\n",
    "Xc = np.load('../dataset/stds.npy')             # Gamaka\n",
    "y = np.load('../dataset/labels.npy')\n",
    "classes = np.load('../dataset/mappings.npy')\n",
    "\n",
    "# Load test split indices to ensure consistent evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "_, Xa_temp, _, Xb_temp, _, Xc_temp, _, y_temp = train_test_split(Xa, Xb, Xc, y, test_size=0.3, random_state=42)\n",
    "_, Xa_test, _, Xb_test, _, Xc_test, _, y_test = train_test_split(Xa_temp, Xb_temp, Xc_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Load all trained models\n",
    "models = {\n",
    "    'Vadi-Samvadi': keras.models.load_model('../models/vadi_samvadi_model.keras'),\n",
    "    'Arohana-Avarohana': keras.models.load_model('../models/arohana_avarohana_model.keras'),\n",
    "    'Gamaka': keras.models.load_model('../models/gamaka_model.keras'),\n",
    "    'Combined': keras.models.load_model('../models/combined_model.keras')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023215b1",
   "metadata": {},
   "source": [
    "### Cross-Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "697dbbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Vadi-Samvadi model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744470488.593224   33664 service.cc:146] XLA service 0x7f43b0004870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744470488.593268   33664 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m118/170\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744470489.519771   33664 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Accuracy: 0.0114, F1 Score: 0.0227\n",
      "\n",
      "Evaluating Arohana-Avarohana model...\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Accuracy: 0.4432, F1 Score: 0.4128\n",
      "\n",
      "Evaluating Gamaka model...\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Accuracy: 0.0228, F1 Score: 0.0244\n",
      "\n",
      "Evaluating Combined model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer \"functional_1\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(32, 73) dtype=float32>, <tf.Tensor 'data_1:0' shape=(32, 73) dtype=float32>, <tf.Tensor 'data_2:0' shape=(32, 73) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Get predictions based on model type\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name == \u001b[33m'\u001b[39m\u001b[33mCombined\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     y_pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mXa_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXb_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXc_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     22\u001b[39m     X_test = model_inputs[model_name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/amplab/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/amplab/lib/python3.12/site-packages/keras/src/layers/input_spec.py:160\u001b[39m, in \u001b[36massert_input_compatibility\u001b[39m\u001b[34m(input_spec, inputs, layer_name)\u001b[39m\n\u001b[32m    158\u001b[39m inputs = tree.flatten(inputs)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) != \u001b[38;5;28mlen\u001b[39m(input_spec):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    161\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLayer \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m input(s),\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m but it received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m input tensors. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInputs received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_index, (x, spec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(inputs, input_spec)):\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Layer \"functional_1\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(32, 73) dtype=float32>, <tf.Tensor 'data_1:0' shape=(32, 73) dtype=float32>, <tf.Tensor 'data_2:0' shape=(32, 73) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "# Evaluate performance metrics for each model\n",
    "results = []\n",
    "\n",
    "# Single feature models\n",
    "model_inputs = {\n",
    "    'Vadi-Samvadi': Xa_test,\n",
    "    'Arohana-Avarohana': Xb_test,\n",
    "    'Gamaka': Xc_test\n",
    "}\n",
    "\n",
    "# Initialize dataframe to store per-class metrics\n",
    "per_class_metrics = {}\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} model...\")\n",
    "    \n",
    "    # Get predictions based on model type\n",
    "    if model_name == 'Combined':\n",
    "        y_pred = model.predict([Xa_test, Xb_test, Xc_test])\n",
    "    else:\n",
    "        X_test = model_inputs[model_name]\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Convert to class indices\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_true_classes, y_pred_classes)\n",
    "    f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
    "    \n",
    "    # Get per-class report\n",
    "    report = classification_report(y_true_classes, y_pred_classes, \n",
    "                                  target_names=classes, output_dict=True)\n",
    "    \n",
    "    # Store per-class F1 scores\n",
    "    for raga in classes:\n",
    "        if raga not in per_class_metrics:\n",
    "            per_class_metrics[raga] = {}\n",
    "        per_class_metrics[raga][model_name] = report[raga]['f1-score']\n",
    "    \n",
    "    # Store overall results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': acc,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Convert results to dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overall performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot accuracy and F1 score for each model\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, results_df['Accuracy'], width, label='Accuracy')\n",
    "rects2 = ax.bar(x + width/2, results_df['F1 Score'], width, label='F1 Score')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Model'])\n",
    "ax.legend()\n",
    "\n",
    "# Add values on top of bars\n",
    "for rect in rects1:\n",
    "    height = rect.get_height()\n",
    "    ax.annotate(f'{height:.3f}',\n",
    "                xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "for rect in rects2:\n",
    "    height = rect.get_height()\n",
    "    ax.annotate(f'{height:.3f}',\n",
    "                xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/model_performance_comparison.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de59f8",
   "metadata": {},
   "source": [
    "### Per-Raga Model Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace83b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for per-class performance\n",
    "per_class_df = pd.DataFrame(per_class_metrics).T\n",
    "per_class_df = per_class_df.reset_index().rename(columns={'index': 'Raga'})\n",
    "\n",
    "# Melt the dataframe for easier plotting with seaborn\n",
    "melted_df = pd.melt(per_class_df, id_vars=['Raga'], var_name='Model', value_name='F1 Score')\n",
    "\n",
    "# Plot heatmap of per-class F1 scores across models\n",
    "plt.figure(figsize=(14, 14))\n",
    "heatmap_df = per_class_df.set_index('Raga')\n",
    "sns.heatmap(heatmap_df, annot=True, cmap='viridis', fmt='.2f', linewidths=.5)\n",
    "plt.title('F1 Score by Raga and Model')\n",
    "plt.savefig('../plots/f1_score_heatmap.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot best model for each raga\n",
    "best_models = per_class_df.set_index('Raga').idxmax(axis=1).reset_index()\n",
    "best_models.columns = ['Raga', 'Best Model']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='Best Model', data=best_models)\n",
    "plt.title('Count of Ragas Where Each Model Performs Best')\n",
    "plt.savefig('../plots/best_model_count.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027e971",
   "metadata": {},
   "source": [
    "**Most Confused Ragas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73166b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances across all ragas\n",
    "model_comparison = []\n",
    "\n",
    "# Get sorted list of ragas by F1 score for better visualization\n",
    "avg_f1_by_raga = per_class_df.iloc[:, 1:].mean(axis=1)\n",
    "sorted_ragas = per_class_df.loc[avg_f1_by_raga.sort_values().index, 'Raga'].tolist()\n",
    "\n",
    "# Create comparison data for all models across all ragas\n",
    "for model_name in per_class_df.columns[1:]:  # Skip 'Raga' column\n",
    "    for raga in sorted_ragas:\n",
    "        f1_score = per_class_df.loc[per_class_df['Raga'] == raga, model_name].values[0]\n",
    "        model_comparison.append({\n",
    "            'Model': model_name,\n",
    "            'Raga': raga,\n",
    "            'F1 Score': f1_score\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "model_comparison_df = pd.DataFrame(model_comparison)\n",
    "\n",
    "# Plot model comparison for all ragas\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.barplot(x='Raga', y='F1 Score', hue='Model', data=model_comparison_df)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Model Performance Comparison Across All Ragas')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/model_comparison_by_raga.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of F1 scores for each model\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='F1 Score', data=model_comparison_df)\n",
    "plt.title('Distribution of F1 Scores by Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/f1_score_distribution.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Find where each model performs best\n",
    "best_ragas_by_model = {}\n",
    "for model in per_class_df.columns[1:]:\n",
    "    # Get top 5 ragas for this model\n",
    "    top_ragas = per_class_df.sort_values(by=model, ascending=False).head(5)['Raga'].tolist()\n",
    "    best_ragas_by_model[model] = top_ragas\n",
    "\n",
    "# Print best ragas for each model\n",
    "print(\"\\nTop 5 ragas for each model:\")\n",
    "for model, ragas in best_ragas_by_model.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    for i, raga in enumerate(ragas, 1):\n",
    "        f1 = per_class_df.loc[per_class_df['Raga'] == raga, model].values[0]\n",
    "        print(f\"  {i}. {raga} (F1: {f1:.3f})\")\n",
    "\n",
    "# Plot top 5 ragas for each model\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, (model, ragas) in enumerate(best_ragas_by_model.items()):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    \n",
    "    # Get F1 scores for these ragas\n",
    "    model_data = per_class_df.loc[per_class_df['Raga'].isin(ragas), ['Raga', model]]\n",
    "    model_data = model_data.sort_values(by=model, ascending=False)\n",
    "    \n",
    "    sns.barplot(x='Raga', y=model, data=model_data)\n",
    "    plt.title(f'Top 5 Ragas for {model} Model')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for j, v in enumerate(model_data[model]):\n",
    "        plt.text(j, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/top_ragas_by_model.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify most commonly confused raga pairs for each model\n",
    "confused_pairs_by_model = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nAnalyzing confused pairs for {model_name} model...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    if model_name == 'Combined':\n",
    "        y_pred = model.predict([Xa_test, Xb_test, Xc_test])\n",
    "    else:\n",
    "        X_test = model_inputs[model_name]\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Convert to class indices\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Find confusion pairs\n",
    "    confusion_pairs = []\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                confusion_pairs.append({\n",
    "                    'True Raga': classes[i],\n",
    "                    'Predicted Raga': classes[j],\n",
    "                    'Count': cm[i, j],\n",
    "                    'Error Rate': cm[i, j] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame and sort\n",
    "    confusion_df = pd.DataFrame(confusion_pairs)\n",
    "    confusion_df = confusion_df.sort_values('Count', ascending=False).head(5)\n",
    "    confused_pairs_by_model[model_name] = confusion_df\n",
    "    \n",
    "    # Print top confusions\n",
    "    print(f\"Top 10 most confused raga pairs for {model_name} model:\")\n",
    "    for idx, row in confusion_df.iterrows():\n",
    "        print(f\"  {row['True Raga']} → {row['Predicted Raga']}: {row['Count']} instances ({row['Error Rate']:.2%} error rate)\")\n",
    "    \n",
    "\n",
    "# Create a visualization comparing the top 3 confused pairs across all models\n",
    "common_confusions = {}\n",
    "\n",
    "# Find the most frequent confusions across all models\n",
    "all_confusions = []\n",
    "for model_name, conf_df in confused_pairs_by_model.items():\n",
    "    for _, row in conf_df.iterrows():\n",
    "        confusion_key = f\"{row['True Raga']} → {row['Predicted Raga']}\"\n",
    "        all_confusions.append({\n",
    "            'Confusion': confusion_key,\n",
    "            'Model': model_name,\n",
    "            'Count': row['Count'],\n",
    "            'Error Rate': row['Error Rate']\n",
    "        })\n",
    "\n",
    "all_conf_df = pd.DataFrame(all_confusions)\n",
    "\n",
    "# Get top 5 most common confusions across all models\n",
    "top_confusions = all_conf_df.groupby('Confusion')['Count'].sum().sort_values(ascending=False).head(5).index.tolist()\n",
    "\n",
    "# Filter data for these top confusions\n",
    "top_conf_data = all_conf_df[all_conf_df['Confusion'].isin(top_confusions)]\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.barplot(x='Confusion', y='Count', hue='Model', data=top_conf_data)\n",
    "plt.title('Top 5 Confused Raga Pairs Across All Models')\n",
    "plt.xlabel('Confusion Pair')\n",
    "plt.ylabel('Number of Misclassifications')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/top_confusions_across_models.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696c782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
